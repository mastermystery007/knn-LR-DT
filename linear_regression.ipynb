{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitbf7420ae57154df185dcfbca0c301360",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       1    3     5        6     7     8     9\n",
       "0    5.0  8.0  56.0  17590.0  6.55  6.44  4.03\n",
       "1    4.0  2.0  59.0  17591.0  8.90  8.82  5.29\n",
       "2    4.0  4.0  60.0  17592.0  8.12  8.06  4.94\n",
       "3    5.0  5.0  55.0  17595.0  7.72  7.65  4.78\n",
       "4    3.0  4.0  60.0  17597.0  7.65  7.68  4.66\n",
       "..   ...  ...   ...      ...   ...   ...   ...\n",
       "418  5.0  5.0  57.0  18571.0  7.60  7.63  4.61\n",
       "419  2.0  3.0  59.0  18572.0  7.98  8.03  5.05\n",
       "420  1.0  3.0  64.0  18574.0  7.43  6.64  4.69\n",
       "421  4.0  2.0  60.0  18575.0  8.28  8.21  5.02\n",
       "422  5.0  2.0  55.0  18578.0  8.19  8.14  5.03\n",
       "\n",
       "[423 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>3</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.0</td>\n      <td>8.0</td>\n      <td>56.0</td>\n      <td>17590.0</td>\n      <td>6.55</td>\n      <td>6.44</td>\n      <td>4.03</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>59.0</td>\n      <td>17591.0</td>\n      <td>8.90</td>\n      <td>8.82</td>\n      <td>5.29</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>60.0</td>\n      <td>17592.0</td>\n      <td>8.12</td>\n      <td>8.06</td>\n      <td>4.94</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>55.0</td>\n      <td>17595.0</td>\n      <td>7.72</td>\n      <td>7.65</td>\n      <td>4.78</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>60.0</td>\n      <td>17597.0</td>\n      <td>7.65</td>\n      <td>7.68</td>\n      <td>4.66</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>418</th>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>57.0</td>\n      <td>18571.0</td>\n      <td>7.60</td>\n      <td>7.63</td>\n      <td>4.61</td>\n    </tr>\n    <tr>\n      <th>419</th>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>59.0</td>\n      <td>18572.0</td>\n      <td>7.98</td>\n      <td>8.03</td>\n      <td>5.05</td>\n    </tr>\n    <tr>\n      <th>420</th>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>64.0</td>\n      <td>18574.0</td>\n      <td>7.43</td>\n      <td>6.64</td>\n      <td>4.69</td>\n    </tr>\n    <tr>\n      <th>421</th>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>60.0</td>\n      <td>18575.0</td>\n      <td>8.28</td>\n      <td>8.21</td>\n      <td>5.02</td>\n    </tr>\n    <tr>\n      <th>422</th>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>55.0</td>\n      <td>18578.0</td>\n      <td>8.19</td>\n      <td>8.14</td>\n      <td>5.03</td>\n    </tr>\n  </tbody>\n</table>\n<p>423 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "headers = [\"carat\",\t\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"]\n",
    "data = pd.read_csv('diamonds1.1.csv', na_values='?',    \n",
    "         header=None,  names = headers) \n",
    "\n",
    "data = data.iloc[1:,:]\n",
    "\n",
    "d_cut = {\"Ideal\":5,\"Premium\":4,\"Very Good\":3,\"Good\":2,\"Fair\":1}\n",
    "d_color = {\"D\":7,\"E\":6,\"F\":5,\"G\":4,\"H\":3,\"I\":2,\"J\":1}\n",
    "d_clarity = {\"IF\":8,\"VVS1\":7,\"VVS2\":6,\"VS1\":5,\"VS2\":4,\"SI1\":3,\"SI2\":2,\"I1\":4}\n",
    " \n",
    "\n",
    "\n",
    "data[\"cut\"] = data[\"cut\"].map(d_cut)\n",
    "data[\"color\"] = data[\"color\"].map(d_color)\n",
    "data[\"clarity\"] = data[\"clarity\"].map(d_clarity)\n",
    "# data[\"x\"] = data[\"x\"]\n",
    "# data[\"y\"] = data[\"y\"]\n",
    "# data[\"z\"] = data[\"z\"]\n",
    "# data[\"price\"] = data[\"price\"]\n",
    "# data[\"carat\"] = data[\"carat\"]\n",
    "# data[\"table\"] = data[\"table\"]\n",
    "# data[\"depth\"] = data[\"depth\"]\n",
    "n = data.values.astype(np.double)\n",
    "data_final = pd.DataFrame(n)\n",
    "\n",
    "corr_matrix = data_final.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\n",
    "#print(corr_matrix)\n",
    "data_final.drop(data_final.columns[[8,9]], axis = 1)\n",
    "data_final.drop(data_final.columns[[0, 4, 2]], axis = 1, inplace = True)\n",
    "data_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.17258795, -0.33035484, -0.47753909,  0.06570808,\n",
       "         0.18872772,  0.79355595,  0.89082416,  0.88622763,  0.84936391],\n",
       "       [-0.17258795,  1.        ,  0.00936876,  0.05404531, -0.13207139,\n",
       "        -0.44383476, -0.08260832, -0.10592798, -0.10936237, -0.12430541],\n",
       "       [-0.33035484,  0.00936876,  1.        ,  0.02761178, -0.10405707,\n",
       "         0.03665027, -0.01620278, -0.22210091, -0.22002614, -0.21786718],\n",
       "       [-0.47753909,  0.05404531,  0.02761178,  1.        , -0.05342556,\n",
       "        -0.0826684 , -0.22557867, -0.39013419, -0.39171977, -0.38037902],\n",
       "       [ 0.06570808, -0.13207139, -0.10405707, -0.05342556,  1.        ,\n",
       "        -0.33328891, -0.05843518, -0.08595377, -0.09630548,  0.07145125],\n",
       "       [ 0.18872772, -0.44383476,  0.03665027, -0.0826684 , -0.33328891,\n",
       "         1.        ,  0.16638977,  0.18641348,  0.17980397,  0.14860275],\n",
       "       [ 0.79355595, -0.08260832, -0.01620278, -0.22557867, -0.05843518,\n",
       "         0.16638977,  1.        ,  0.84853283,  0.84817186,  0.78502459],\n",
       "       [ 0.89082416, -0.10592798, -0.22210091, -0.39013419, -0.08595377,\n",
       "         0.18641348,  0.84853283,  1.        ,  0.99759456,  0.92763934],\n",
       "       [ 0.88622763, -0.10936237, -0.22002614, -0.39171977, -0.09630548,\n",
       "         0.17980397,  0.84817186,  0.99759456,  1.        ,  0.92769616],\n",
       "       [ 0.84936391, -0.12430541, -0.21786718, -0.38037902,  0.07145125,\n",
       "         0.14860275,  0.78502459,  0.92763934,  0.92769616,  1.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "R3 = np.corrcoef(n, None, rowvar=False)\n",
    "R3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[4.1]\n [3.7]\n [1.3]\n [3.2]\n [3.9]\n [2. ]\n [1.1]\n [2.9]\n [4. ]]****\n[[3.2]\n [3. ]\n [2.2]\n [4. ]\n [1.5]]****"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:,:-1].values \n",
    "Y = data.iloc[:,1].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(  \n",
    "X, Y, test_size = 1/3, random_state = 0 ) \n",
    "weights  = np.zeros(n) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dimensions of weight are  2\nvalues of W are  [0. 0.]\nlength of X is  8\nlength of y is  8\nX is  [[3.7000e+00 5.7189e+04]\n [1.5000e+00 3.7731e+04]\n [3.2000e+00 6.4445e+04]\n [3.9000e+00 6.3218e+04]\n [2.2000e+00 3.9891e+04]\n [1.3000e+00 4.6205e+04]\n [3.0000e+00 6.0150e+04]\n [4.1000e+00 5.7081e+04]]\nY is  [57189 37731 64445 63218 39891 46205 60150 57081]\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n33521819430178.84\n55638166405880.25\n93413152004495.31\n130468921127724.1\n153851370518174.88\n180934824743146.2\n216192259513634.6\n249650773837031.62\n-1.900261099831195e+22\n-3.1539768749203875e+22\n-5.295338438850012e+22\n-7.3959295698316586e+22\n-8.721417259675042e+22\n-1.0256704883957604e+23\n-1.2255353313966052e+23\n-1.4152025818836693e+23\n1.0772065226062338e+31\n1.7879040212501717e+31\n3.001783863408802e+31\n4.19255205196102e+31\n4.943935104142848e+31\n5.814248053821562e+31\n6.947227687721904e+31\n8.022399933091791e+31\n-6.10639186608879e+39\n-1.0135143395061268e+40\n-1.701629927279649e+40\n-2.3766441449229e+40\n-2.8025828355891177e+40\n-3.2959396622830195e+40\n-3.938195114297423e+40\n-4.547681124267589e+40\n3.461548072696339e+48\n5.745338139942628e+48\n9.646078935628585e+48\n1.3472551614366506e+49\n1.5887082627271876e+49\n1.8683788783779498e+49\n2.2324560897415822e+49\n2.5779571924244886e+49\n-1.962257798444e+57\n-3.256876499484384e+57\n-5.46810075097404e+57\n-7.637224419546321e+57\n-9.00595661975173e+57\n-1.0591333552358854e+58\n-1.2655188602846558e+58\n-1.4613740727134817e+58\n1.1123507710106767e+66\n1.8462350299541422e+66\n3.0997181364921537e+66\n4.329335563451633e+66\n5.105232756681249e+66\n6.003939977886569e+66\n7.17388347791226e+66\n8.28413360266355e+66\n-6.30561508660687e+74\n-1.0465806076372275e+75\n-1.757146212784551e+75\n-2.454183010911245e+75\n-2.894018103832454e+75\n-3.4034708736027203e+75\n-4.066680139645367e+75\n-4.696050848866703e+75\n3.5744823176881243e+83\n5.93278185339986e+83\n9.96078571388165e+83\n1.3912098433514442e+84\n1.6405404385038332e+84\n1.9293354081030599e+84\n2.305290768815067e+84\n2.662063968651005e+84\n-2.026277161541823e+92\n-3.3631332611344546e+92\n-5.646499495374328e+92\n-7.886391600108527e+92\n-9.299779178306631e+92\n-1.0936879600852727e+93\n-1.3068068661154647e+93\n-1.509051924959489e+93\n1.1486416130997965e+101\n1.9064691086976576e+101\n3.200847550292091e+101\n4.4705816859686165e+101\n5.2717928028729595e+101\n6.199820668877654e+101\n7.407933994392877e+101\n8.554406426897112e+101\n-6.511338036009694e+109\n-1.080725674602739e+110\n-1.8144737369770498e+110\n-2.5342516101588166e+110\n-2.98843648043304e+110\n-3.514510329184353e+110\n-4.1993570349386897e+110\n-4.849261188842332e+110\n3.691101082849501e+118\n6.126340985107301e+118\n1.028575991342987e+119\n1.4365985624980338e+119\n1.6940636575694012e+119\n1.9922806664308788e+119\n2.3805017053657374e+119\n2.748914758559364e+119\n-2.0923851792775325e+127\n-3.472856687669775e+127\n-5.830718562671543e+127\n-8.143687949130209e+127\n-9.603187803013173e+127\n-1.1293699212872804e+128\n-1.3494419079162483e+128\n-1.5582852842021208e+128\n1.1861164569029988e+136\n1.9686683458219946e+136\n3.3052763474182466e+136\n4.61643601379417e+136\n5.443786930194677e+136\n6.402092037531149e+136\n7.649620492756038e+136\n8.833496998770051e+136\n-6.723772770278816e+144\n-1.1159847366008077e+145\n-1.8736715921676912e+145\n-2.616932476118757e+145\n-3.085935290368883e+145\n-3.6291724867527987e+145\n-4.336362561434908e+145\n-5.007470071003913e+145\n3.811524576969939e+153\n6.326215052774484e+153\n1.0621336512568397e+154\n1.483468104854446e+154\n1.7493330908165307e+154\n2.0572795363439694e+154\n2.458166425644408e+154\n2.838599086578202e+154\n-2.16064999475636e+162\n-3.5861598802731786e+162\n-6.02094784298368e+162\n-8.40937868364233e+162\n-9.916495242711046e+162\n-1.1662160192464688e+163\n-1.3934679332177212e+163\n-1.6091248994139624e+163\n1.2248139309000386e+171\n2.032896855322788e+171\n3.4131121714327082e+171\n4.7670488912760716e+171\n5.621392427488491e+171\n6.610962581993444e+171\n7.899192099644095e+171\n9.121693000455566e+171\n-6.943138263798098e+179\n-1.1523941381181454e+180\n-1.9348007985748082e+180\n-2.702310834927665e+180\n-3.186615033880244e+180\n-3.747575538257188e+180\n-4.477837942276529e+180\n-5.170840574579581e+180\n3.935876930693698e+188\n6.532610083447634e+188\n1.0967861418379175e+189\n1.5318667828079867e+189\n1.8064057091079826e+189\n2.124399017655427e+189\n2.5383649852235797e+189\n2.931209397902686e+189\n-2.231141974276737e+197\n-3.7031596300940746e+197\n-6.21738342166181e+197\n-8.683737673476432e+197\n-1.0240024449782796e+198\n-1.204264234341264e+198\n-1.43893032335452e+198\n-1.661623175270996e+198\n1.2647739238386518e+206\n2.0992208429377343e+206\n3.5244661778073116e+206\n4.922575567800264e+206\n5.804792367708428e+206\n6.826647602737589e+206\n8.156906069545281e+206\n9.419291499860737e+206\n-7.16966063506906e+214\n-1.1899914093933513e+215\n-1.9979243672231976e+215\n-2.7904746932553508e+215\n-3.290579490063691e+215\n-3.8698415317012964e+215\n-4.623929008061628e+215\n-5.339541099315659e+215\n4.064286324471736e+223\n6.745738825879111e+223\n1.132569182328652e+224\n1.5818444849548925e+224\n1.8653403420013084e+224\n2.193708296071232e+224\n2.6211800515174615e+224\n3.0268411537855695e+224\n-2.3039337798627708e+232\n-3.82397653863493e+232\n-6.420227781411773e+232\n-8.967047723565341e+232\n-1.0574108913048048e+233\n-1.243553786073533e+233\n-1.485875940243568e+233\n-1.715834226170517e+233\n1.3060376258512463e+241\n2.1677086743904172e+241\n3.639453148500946e+241\n5.083176357819477e+241\n5.994175796629169e+241\n7.049369424491635e+241\n8.423028049967594e+241\n9.726599256839448e+241\n-7.403573379790277e+249\n-1.2288153051024941e+250\n-2.0631073648948962e+250\n-2.8815149290207305e+250\n-3.397935823845969e+250\n-3.9960964969487105e+250\n-4.7747863471637486e+250\n-5.513745539060473e+250\n4.196885120688109e+258\n6.9658209698254744e+258\n1.1695196573245447e+259\n1.6334527275247095e+259\n1.926197738389433e+259\n2.2652788145057874e+259\n2.7066969889942426e+259\n3.125592929937145e+259\n-2.3791004441631135e+267\n-3.948735142065398e+267\n-6.629690011010168e+267\n-9.259600865454053e+267\n-1.0919093001518547e+268\n-1.2841251734954309e+268\n-1.534353174688604e+268\n-1.771813932011403e+268\n1.3486475709130463e+276\n2.2384309458606293e+276\n3.758191610275005e+276\n5.249016806102855e+276\n6.189737927711472e+276\n7.279357625407474e+276\n8.697832355264077e+276\n1.0043933039395632e+277\n-7.64511761154107e+284\n-1.2689058443068217e+285\n-2.1304169811989979e+285\n-2.9755253850673584e+285\n-3.508794696447922e+285\n-4.126470575632437e+285\n-4.93056546095603e+285\n-5.693633461011409e+285\n4.333810000097042e+293\n7.193083372500243e+293\n1.2076756547942308e+294\n1.6867447074824174e+294\n1.9890406291196635e+294\n2.339184346724158e+294\n2.79500394716845e+294\n3.227566518135549e+294\n-2.4567194477934447e+302\n-4.077564039592255e+302\n-6.8459860208297895e+302\n-9.561698658321272e+302\n-1.1275332319368333e+303\n-1.326020216955188e+303\n-1.5844119962606617e+303\n-1.829619995794238e+303\ninf\ninf\ninf\ninf\ninf\ninf\ninf\ninf\nPredicted values  nan\nReal values       [54445 56957 56642 55794 43525]\nTrained W         nan\nTrained b         nan\n"
     ]
    }
   ],
   "source": [
    " \n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd \n",
    "  \n",
    "from sklearn.model_selection import train_test_split \n",
    "  \n",
    "import matplotlib.pyplot as plt \n",
    "  \n",
    "# Linear Regression \n",
    "  \n",
    "class LinearRegression() : \n",
    "      \n",
    "    def __init__( self, learning_rate, iterations ) : \n",
    "          \n",
    "        self.learning_rate = learning_rate \n",
    "          \n",
    "        self.iterations = iterations \n",
    "          \n",
    "    # Function for model training \n",
    "              \n",
    "    def fit( self, X, Y ) : \n",
    "          \n",
    "        # no_of_training_examples, no_of_features \n",
    "        self.m, self.n = X.shape \n",
    "        \n",
    "        # weight initialization \n",
    "        self.W = np.zeros( self.n ) \n",
    "         \n",
    "        self.b = 0\n",
    "          \n",
    "        self.X = X \n",
    "          \n",
    "        self.Y = Y \n",
    "        \n",
    "        # gradient descent learning \n",
    "                  \n",
    "        for i in range( self.iterations ) : \n",
    "            self.update_weights() \n",
    "        return self\n",
    "      \n",
    "    # Helper function to update weights in gradient descent \n",
    "      \n",
    "    def update_weights( self ) : \n",
    "\n",
    "        Y_pred = self.predict( self.X ) \n",
    "        # calculate gradients   \n",
    "      \n",
    "        dW = - ( 2 * ( self.X.T ).dot( self.Y - Y_pred )  ) / self.m \n",
    "        db = - 2 * np.sum( self.Y - Y_pred ) / self.m  \n",
    "          \n",
    "        # update weights \n",
    "        self.W = self.W - self.learning_rate * dW \n",
    "        self.b = self.b - self.learning_rate * db \n",
    "          \n",
    "        return self\n",
    "      \n",
    "    # Hypothetical function  h( x )  \n",
    "      \n",
    "    def predict( self, X ) :\n",
    "        Wa = 0\n",
    "        for l in X:\n",
    "            Wa =Wa + l.dot(self.W)\n",
    "            if not math.isnan(Wa):\n",
    "                print(Wa)\n",
    "        a = (Wa/len(X))+self.b         \n",
    "        return a \n",
    "    \n",
    "    \n",
    "    def cost_function(X, Y, B):\n",
    "        m = len(Y)\n",
    "        J = np.sum((X.dot(B) — Y) ** 2)/(2 * m)\n",
    "        return J\n",
    "     \n",
    "  \n",
    "# driver code \n",
    "  \n",
    "def main() : \n",
    "      \n",
    "    # Importing dataset \n",
    "      \n",
    "    df = pd.read_csv( \"salaries.csv\" ) \n",
    "  \n",
    "    X = df.iloc[:,:-1].values \n",
    "  \n",
    "    Y = df.iloc[:,1].values \n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(  \n",
    "      X, Y, test_size = 1/3, random_state = 0 ) \n",
    "      \n",
    "    # Model training \n",
    "      \n",
    "    model = LinearRegression( iterations = 1000, learning_rate = 0.1 ) \n",
    "  \n",
    "    model.fit( X_train, Y_train ) \n",
    "      \n",
    "    # Prediction on test set \n",
    "  \n",
    "    Y_pred = model.predict( X_test ) \n",
    "      \n",
    "    print( \"Predicted values \", Y_pred)  \n",
    "      \n",
    "    print( \"Real values      \", Y_test) \n",
    "      \n",
    "    print( \"Trained W        \", round( model.W[0], 2 ) ) \n",
    "      \n",
    "    print( \"Trained b        \", round( model.b, 2 ) ) \n",
    "      \n",
    "    # Visualization on test set  \n",
    "      \n",
    "    \n",
    "if __name__ == \"__main__\" :  \n",
    "      \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "from numpy import asarray\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder,LabelEncoder\n",
    "\n",
    "headers = [\"carat\",\t\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"]\n",
    "data = pd.read_csv('diamonds1.1.csv', na_values='?',    \n",
    "         header=None,  names = headers) \n",
    "#data = data.reset_index(drop=True)\n",
    "#print(type(data))\n",
    "\n",
    "data = data.iloc[1:,:]\n",
    "\n",
    "\n",
    "d_cut = {\"Ideal\":5,\"Premium\":4,\"Very Good\":3,\"Good\":2,\"Fair\":1}\n",
    "d_color = {\"D\":7,\"E\":6,\"F\":5,\"G\":4,\"H\":3,\"I\":2,\"J\":1}\n",
    "d_clarity = {\"IF\":8,\"VVS1\":7,\"VVS2\":6,\"VS1\":5,\"VS2\":4,\"SI1\":3,\"SI2\":2,\"I1\":4}\n",
    " \n",
    "\n",
    "\n",
    "data[\"cut\"] = data[\"cut\"].map(d_cut)\n",
    "data[\"color\"] = data[\"color\"].map(d_color)\n",
    "data[\"clarity\"] = data[\"clarity\"].map(d_clarity)\n",
    "\n",
    "\n",
    "max_price_global = data[\"price\"].max()\n",
    "\n",
    "n = data.values.astype(numpy.double)\n",
    "\n",
    "n = n / n.max(axis=0) #normalise data\n",
    "\n",
    "X_train , X_test = train_test_split(n, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.8:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "selected_columns = data.columns[columns]\n",
    "data = data[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     carat  cut  color  clarity  depth  table  price     x\n0     1.03    5      7        8   62.0   56.0  17590  6.55\n1     2.53    4      3        2   59.6   59.0  17591  8.90\n2     2.01    4      2        4   61.1   60.0  17592  8.12\n3     1.71    5      4        5   62.2   55.0  17595  7.72\n4     1.70    3      5        4   60.8   60.0  17597  7.65\n..     ...  ...    ...      ...    ...    ...    ...   ...\n418   1.60    5      5        5   60.5   57.0  18571  7.60\n419   2.01    2      4        3   63.1   59.0  18572  7.98\n420   2.01    1      4        3   70.6   64.0  18574  7.43\n421   2.11    4      7        2   60.9   60.0  18575  8.28\n422   2.03    5      5        2   61.6   55.0  18578  8.19\n\n[423 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8\n423\n"
     ]
    }
   ],
   "source": [
    "iterations = 1000\n",
    "learning_rate = 0.01\n",
    "rows , cols = data.shape\n",
    "print(cols)\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[95 95]\n(2,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def cost_function(X, Y, B):\n",
    "    m = len(Y)\n",
    "    J = X.dot(W)\n",
    "    return J\n",
    "\n",
    "X = np.array([[1, 2, 3, 4, 5],[1, 2, 3, 4, 5]])\n",
    "\n",
    "w = np.array([6,7,8,9,3])\n",
    "loss = X.dot(w)\n",
    "print(loss)\n",
    "m  = loss.T.shape\n",
    "print (m)\n",
    "# g = X.dot(loss)\n",
    "# print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   carat        cut color clarity  depth  table  price     x     y     z\n0   1.03      Ideal     D      IF   62.0   56.0  17590  6.55  6.44  4.03\n1   2.53    Premium     H     SI2   59.6   59.0  17591  8.90  8.82  5.29\n2   2.01    Premium     I     VS2   61.1   60.0  17592  8.12  8.06  4.94\n3   1.71      Ideal     G     VS1   62.2   55.0  17595  7.72  7.65  4.78\n4   1.70  Very Good     F     VS2   60.8   60.0  17597  7.65  7.68  4.66\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "data = pd.read_csv(\"diamonds1.1.csv\")\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-62-e1293e5d57c2>, line 10)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-62-e1293e5d57c2>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    J = np.sum((X.dot(B) — Y) ** 2)/(2 * m)\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = data[:,:4]\n",
    "y = data[:,6]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "def cost_function(X, Y, B):\n",
    " m = len(Y)\n",
    " J = np.sum((X.dot(B) — Y) ** 2)/(2 * m)\n",
    " return J\n",
    "\n",
    "\n",
    "def batch_gradient_descent(X, Y, B, alpha, iterations):\n",
    " cost_history = [0] * iterations\n",
    " m = len(Y)\n",
    " \n",
    " for iteration in range(iterations):\n",
    " #print(iteration)\n",
    " # Hypothesis Values\n",
    "    h = X.dot(B)\n",
    "    # Difference b/w Hypothesis and Actual Y\n",
    "    loss = h — Y\n",
    "    # Gradient Calculation\n",
    "    gradient = X.T.dot(loss) / m\n",
    "    # Changing Values of B using Gradient\n",
    "    B = B — alpha * gradient\n",
    "    # New Cost Value\n",
    "    cost = cost_function(X, Y, B)\n",
    "    cost_history[iteration] = cost\n",
    "    \n",
    "    return B, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}