{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitbf7420ae57154df185dcfbca0c301360",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    carat  cut  color  clarity depth table  price     x     y     z\n1    1.03    5      7        8    62    56  17590  6.55  6.44  4.03\n2    2.53    4      3        2  59.6    59  17591   8.9  8.82  5.29\n3    2.01    4      2        4  61.1    60  17592  8.12  8.06  4.94\n4    1.71    5      4        5  62.2    55  17595  7.72  7.65  4.78\n5     1.7    3      5        4  60.8    60  17597  7.65  7.68  4.66\n..    ...  ...    ...      ...   ...   ...    ...   ...   ...   ...\n419   1.6    5      5        5  60.5    57  18571   7.6  7.63  4.61\n420  2.01    2      4        3  63.1    59  18572  7.98  8.03  5.05\n421  2.01    1      4        3  70.6    64  18574  7.43  6.64  4.69\n422  2.11    4      7        2  60.9    60  18575  8.28  8.21  5.02\n423  2.03    5      5        2  61.6    55  18578  8.19  8.14  5.03\n\n[423 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "headers = [\"carat\",\t\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"]\n",
    "data = pd.read_csv('diamonds1.1.csv', na_values='?',    \n",
    "         header=None,  names = headers) \n",
    "data = data.iloc[1:,:]\n",
    "\n",
    "\n",
    "d_cut = {\"Ideal\":5,\"Premium\":4,\"Very Good\":3,\"Good\":2,\"Fair\":1}\n",
    "d_color = {\"D\":7,\"E\":6,\"F\":5,\"G\":4,\"H\":3,\"I\":2,\"J\":1}\n",
    "d_clarity = {\"IF\":8,\"VVS1\":7,\"VVS2\":6,\"VS1\":5,\"VS2\":4,\"SI1\":3,\"SI2\":2,\"I1\":4}\n",
    " \n",
    "#label_encoder = LabelEncoder()\n",
    "#data[:,3] =label_encoder.fit(data[:,3].values)\n",
    "#print(label_encoder.classes_)\n",
    "# data[:,1] =label_encoder.fit_transform(data[:,1])\n",
    "# data[:,2] =label_encoder.fit_transform(data[:,2])\n",
    "# data[:,3] =label_encoder.fit_transform(data[:,3])\n",
    "\n",
    "\n",
    "data[\"cut\"] = data[\"cut\"].map(d_cut)\n",
    "data[\"color\"] = data[\"color\"].map(d_color)\n",
    "data[\"clarity\"] = data[\"clarity\"].map(d_clarity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Cannot describe a DataFrame without columns",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-993e36485aed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcorr_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcorr_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# Select upper triangle of correlation matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mupper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorr_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(self, percentiles, include, exclude, datetime_is_numeric)\u001b[0m\n\u001b[1;32m   9969\u001b[0m         \"\"\"\n\u001b[1;32m   9970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9971\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot describe a DataFrame without columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9973\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpercentiles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot describe a DataFrame without columns"
     ]
    }
   ],
   "source": [
    "X = data[['carat', 'cut','color', 'clarity','x','y','z']].values \n",
    "Y = data[['price']].values\n",
    "X_f = pd.DataFrame(X)\n",
    "Y_f = pd.DataFrame(Y)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(  \n",
    "X, Y, test_size = 1/3, random_state = 0 ) \n",
    "\n",
    "\n",
    "print(type(corr_matrix))\n",
    "corr_matrix.describe()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]\n",
    "X_f.drop(X_f[to_drop], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Empty DataFrame\nColumns: []\nIndex: []\n"
     ]
    }
   ],
   "source": [
    "# def correlation(dataset, threshold):\n",
    "#     col_corr = set() # Set of all the names of deleted columns\n",
    "#     corr_matrix = dataset.corr()\n",
    "#     for i in range(len(corr_matrix.columns)):\n",
    "#         for j in range(i):\n",
    "#             if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n",
    "#                 colname = corr_matrix.columns[i] # getting the name of column\n",
    "#                 col_corr.add(colname)\n",
    "#                 if colname in dataset.columns:\n",
    "#                     del dataset[colname] # deleting the column from the dataset\n",
    "\n",
    "#     print(dataset)\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# # Find index of feature columns with correlation greater than 0.95\n",
    "# to_drop = [column for column in upper.columns if any(upper[column] > 0.1)]\n",
    "\n",
    "# X_f.drop(X_f[to_drop], axis=1)\n",
    "\n",
    "# print(X_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dimensions of weight are  2\nvalues of W are  [0. 0.]\nlength of X is  8\nlength of y is  8\nX is  [[3.7000e+00 5.7189e+04]\n [1.5000e+00 3.7731e+04]\n [3.2000e+00 6.4445e+04]\n [3.9000e+00 6.3218e+04]\n [2.2000e+00 3.9891e+04]\n [1.3000e+00 4.6205e+04]\n [3.0000e+00 6.0150e+04]\n [4.1000e+00 5.7081e+04]]\nY is  [57189 37731 64445 63218 39891 46205 60150 57081]\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n33521819430178.84\n55638166405880.25\n93413152004495.31\n130468921127724.1\n153851370518174.88\n180934824743146.2\n216192259513634.6\n249650773837031.62\n-1.900261099831195e+22\n-3.1539768749203875e+22\n-5.295338438850012e+22\n-7.3959295698316586e+22\n-8.721417259675042e+22\n-1.0256704883957604e+23\n-1.2255353313966052e+23\n-1.4152025818836693e+23\n1.0772065226062338e+31\n1.7879040212501717e+31\n3.001783863408802e+31\n4.19255205196102e+31\n4.943935104142848e+31\n5.814248053821562e+31\n6.947227687721904e+31\n8.022399933091791e+31\n-6.10639186608879e+39\n-1.0135143395061268e+40\n-1.701629927279649e+40\n-2.3766441449229e+40\n-2.8025828355891177e+40\n-3.2959396622830195e+40\n-3.938195114297423e+40\n-4.547681124267589e+40\n3.461548072696339e+48\n5.745338139942628e+48\n9.646078935628585e+48\n1.3472551614366506e+49\n1.5887082627271876e+49\n1.8683788783779498e+49\n2.2324560897415822e+49\n2.5779571924244886e+49\n-1.962257798444e+57\n-3.256876499484384e+57\n-5.46810075097404e+57\n-7.637224419546321e+57\n-9.00595661975173e+57\n-1.0591333552358854e+58\n-1.2655188602846558e+58\n-1.4613740727134817e+58\n1.1123507710106767e+66\n1.8462350299541422e+66\n3.0997181364921537e+66\n4.329335563451633e+66\n5.105232756681249e+66\n6.003939977886569e+66\n7.17388347791226e+66\n8.28413360266355e+66\n-6.30561508660687e+74\n-1.0465806076372275e+75\n-1.757146212784551e+75\n-2.454183010911245e+75\n-2.894018103832454e+75\n-3.4034708736027203e+75\n-4.066680139645367e+75\n-4.696050848866703e+75\n3.5744823176881243e+83\n5.93278185339986e+83\n9.96078571388165e+83\n1.3912098433514442e+84\n1.6405404385038332e+84\n1.9293354081030599e+84\n2.305290768815067e+84\n2.662063968651005e+84\n-2.026277161541823e+92\n-3.3631332611344546e+92\n-5.646499495374328e+92\n-7.886391600108527e+92\n-9.299779178306631e+92\n-1.0936879600852727e+93\n-1.3068068661154647e+93\n-1.509051924959489e+93\n1.1486416130997965e+101\n1.9064691086976576e+101\n3.200847550292091e+101\n4.4705816859686165e+101\n5.2717928028729595e+101\n6.199820668877654e+101\n7.407933994392877e+101\n8.554406426897112e+101\n-6.511338036009694e+109\n-1.080725674602739e+110\n-1.8144737369770498e+110\n-2.5342516101588166e+110\n-2.98843648043304e+110\n-3.514510329184353e+110\n-4.1993570349386897e+110\n-4.849261188842332e+110\n3.691101082849501e+118\n6.126340985107301e+118\n1.028575991342987e+119\n1.4365985624980338e+119\n1.6940636575694012e+119\n1.9922806664308788e+119\n2.3805017053657374e+119\n2.748914758559364e+119\n-2.0923851792775325e+127\n-3.472856687669775e+127\n-5.830718562671543e+127\n-8.143687949130209e+127\n-9.603187803013173e+127\n-1.1293699212872804e+128\n-1.3494419079162483e+128\n-1.5582852842021208e+128\n1.1861164569029988e+136\n1.9686683458219946e+136\n3.3052763474182466e+136\n4.61643601379417e+136\n5.443786930194677e+136\n6.402092037531149e+136\n7.649620492756038e+136\n8.833496998770051e+136\n-6.723772770278816e+144\n-1.1159847366008077e+145\n-1.8736715921676912e+145\n-2.616932476118757e+145\n-3.085935290368883e+145\n-3.6291724867527987e+145\n-4.336362561434908e+145\n-5.007470071003913e+145\n3.811524576969939e+153\n6.326215052774484e+153\n1.0621336512568397e+154\n1.483468104854446e+154\n1.7493330908165307e+154\n2.0572795363439694e+154\n2.458166425644408e+154\n2.838599086578202e+154\n-2.16064999475636e+162\n-3.5861598802731786e+162\n-6.02094784298368e+162\n-8.40937868364233e+162\n-9.916495242711046e+162\n-1.1662160192464688e+163\n-1.3934679332177212e+163\n-1.6091248994139624e+163\n1.2248139309000386e+171\n2.032896855322788e+171\n3.4131121714327082e+171\n4.7670488912760716e+171\n5.621392427488491e+171\n6.610962581993444e+171\n7.899192099644095e+171\n9.121693000455566e+171\n-6.943138263798098e+179\n-1.1523941381181454e+180\n-1.9348007985748082e+180\n-2.702310834927665e+180\n-3.186615033880244e+180\n-3.747575538257188e+180\n-4.477837942276529e+180\n-5.170840574579581e+180\n3.935876930693698e+188\n6.532610083447634e+188\n1.0967861418379175e+189\n1.5318667828079867e+189\n1.8064057091079826e+189\n2.124399017655427e+189\n2.5383649852235797e+189\n2.931209397902686e+189\n-2.231141974276737e+197\n-3.7031596300940746e+197\n-6.21738342166181e+197\n-8.683737673476432e+197\n-1.0240024449782796e+198\n-1.204264234341264e+198\n-1.43893032335452e+198\n-1.661623175270996e+198\n1.2647739238386518e+206\n2.0992208429377343e+206\n3.5244661778073116e+206\n4.922575567800264e+206\n5.804792367708428e+206\n6.826647602737589e+206\n8.156906069545281e+206\n9.419291499860737e+206\n-7.16966063506906e+214\n-1.1899914093933513e+215\n-1.9979243672231976e+215\n-2.7904746932553508e+215\n-3.290579490063691e+215\n-3.8698415317012964e+215\n-4.623929008061628e+215\n-5.339541099315659e+215\n4.064286324471736e+223\n6.745738825879111e+223\n1.132569182328652e+224\n1.5818444849548925e+224\n1.8653403420013084e+224\n2.193708296071232e+224\n2.6211800515174615e+224\n3.0268411537855695e+224\n-2.3039337798627708e+232\n-3.82397653863493e+232\n-6.420227781411773e+232\n-8.967047723565341e+232\n-1.0574108913048048e+233\n-1.243553786073533e+233\n-1.485875940243568e+233\n-1.715834226170517e+233\n1.3060376258512463e+241\n2.1677086743904172e+241\n3.639453148500946e+241\n5.083176357819477e+241\n5.994175796629169e+241\n7.049369424491635e+241\n8.423028049967594e+241\n9.726599256839448e+241\n-7.403573379790277e+249\n-1.2288153051024941e+250\n-2.0631073648948962e+250\n-2.8815149290207305e+250\n-3.397935823845969e+250\n-3.9960964969487105e+250\n-4.7747863471637486e+250\n-5.513745539060473e+250\n4.196885120688109e+258\n6.9658209698254744e+258\n1.1695196573245447e+259\n1.6334527275247095e+259\n1.926197738389433e+259\n2.2652788145057874e+259\n2.7066969889942426e+259\n3.125592929937145e+259\n-2.3791004441631135e+267\n-3.948735142065398e+267\n-6.629690011010168e+267\n-9.259600865454053e+267\n-1.0919093001518547e+268\n-1.2841251734954309e+268\n-1.534353174688604e+268\n-1.771813932011403e+268\n1.3486475709130463e+276\n2.2384309458606293e+276\n3.758191610275005e+276\n5.249016806102855e+276\n6.189737927711472e+276\n7.279357625407474e+276\n8.697832355264077e+276\n1.0043933039395632e+277\n-7.64511761154107e+284\n-1.2689058443068217e+285\n-2.1304169811989979e+285\n-2.9755253850673584e+285\n-3.508794696447922e+285\n-4.126470575632437e+285\n-4.93056546095603e+285\n-5.693633461011409e+285\n4.333810000097042e+293\n7.193083372500243e+293\n1.2076756547942308e+294\n1.6867447074824174e+294\n1.9890406291196635e+294\n2.339184346724158e+294\n2.79500394716845e+294\n3.227566518135549e+294\n-2.4567194477934447e+302\n-4.077564039592255e+302\n-6.8459860208297895e+302\n-9.561698658321272e+302\n-1.1275332319368333e+303\n-1.326020216955188e+303\n-1.5844119962606617e+303\n-1.829619995794238e+303\ninf\ninf\ninf\ninf\ninf\ninf\ninf\ninf\nPredicted values  nan\nReal values       [54445 56957 56642 55794 43525]\nTrained W         nan\nTrained b         nan\n"
     ]
    }
   ],
   "source": [
    " \n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd \n",
    "  \n",
    "from sklearn.model_selection import train_test_split \n",
    "  \n",
    "import matplotlib.pyplot as plt \n",
    "  \n",
    "# Linear Regression \n",
    "  \n",
    "class LinearRegression() : \n",
    "      \n",
    "    def __init__( self, learning_rate, iterations ) : \n",
    "          \n",
    "        self.learning_rate = learning_rate \n",
    "          \n",
    "        self.iterations = iterations \n",
    "          \n",
    "    # Function for model training \n",
    "              \n",
    "    def fit( self, X, Y ) : \n",
    "          \n",
    "        # no_of_training_examples, no_of_features \n",
    "        self.m, self.n = X.shape \n",
    "        \n",
    "        # weight initialization \n",
    "        self.W = np.zeros( self.n ) \n",
    "         \n",
    "        self.b = 0\n",
    "          \n",
    "        self.X = X \n",
    "          \n",
    "        self.Y = Y \n",
    "        \n",
    "        # gradient descent learning \n",
    "                  \n",
    "        for i in range( self.iterations ) : \n",
    "            self.update_weights() \n",
    "        return self\n",
    "      \n",
    "    # Helper function to update weights in gradient descent \n",
    "      \n",
    "    def update_weights( self ) : \n",
    "\n",
    "        Y_pred = self.predict( self.X ) \n",
    "        # calculate gradients   \n",
    "      \n",
    "        dW = - ( 2 * ( self.X.T ).dot( self.Y - Y_pred )  ) / self.m \n",
    "        db = - 2 * np.sum( self.Y - Y_pred ) / self.m  \n",
    "          \n",
    "        # update weights \n",
    "        self.W = self.W - self.learning_rate * dW \n",
    "        self.b = self.b - self.learning_rate * db \n",
    "          \n",
    "        return self\n",
    "      \n",
    "    # Hypothetical function  h( x )  \n",
    "      \n",
    "    def predict( self, X ) :\n",
    "        Wa = 0\n",
    "        for l in X:\n",
    "            Wa =Wa + l.dot(self.W)\n",
    "            if not math.isnan(Wa):\n",
    "                print(Wa)\n",
    "        a = (Wa/len(X))+self.b         \n",
    "        return a \n",
    "    \n",
    "    \n",
    "    def cost_function(X, Y, B):\n",
    "        m = len(Y)\n",
    "        J = np.sum((X.dot(B) â€” Y) ** 2)/(2 * m)\n",
    "        return J\n",
    "     \n",
    "  \n",
    "# driver code \n",
    "  \n",
    "def main() : \n",
    "      \n",
    "    # Importing dataset \n",
    "      \n",
    "    df = pd.read_csv( \"salaries.csv\" ) \n",
    "  \n",
    "    X = df.iloc[:,:-1].values \n",
    "  \n",
    "    Y = df.iloc[:,1].values \n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(  \n",
    "      X, Y, test_size = 1/3, random_state = 0 ) \n",
    "      \n",
    "    # Model training \n",
    "      \n",
    "    model = LinearRegression( iterations = 1000, learning_rate = 0.1 ) \n",
    "  \n",
    "    model.fit( X_train, Y_train ) \n",
    "      \n",
    "    # Prediction on test set \n",
    "  \n",
    "    Y_pred = model.predict( X_test ) \n",
    "      \n",
    "    print( \"Predicted values \", Y_pred)  \n",
    "      \n",
    "    print( \"Real values      \", Y_test) \n",
    "      \n",
    "    print( \"Trained W        \", round( model.W[0], 2 ) ) \n",
    "      \n",
    "    print( \"Trained b        \", round( model.b, 2 ) ) \n",
    "      \n",
    "    # Visualization on test set  \n",
    "      \n",
    "    \n",
    "if __name__ == \"__main__\" :  \n",
    "      \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "from numpy import asarray\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder,LabelEncoder\n",
    "\n",
    "headers = [\"carat\",\t\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"]\n",
    "data = pd.read_csv('diamonds1.1.csv', na_values='?',    \n",
    "         header=None,  names = headers) \n",
    "#data = data.reset_index(drop=True)\n",
    "#print(type(data))\n",
    "\n",
    "data = data.iloc[1:,:]\n",
    "\n",
    "\n",
    "d_cut = {\"Ideal\":5,\"Premium\":4,\"Very Good\":3,\"Good\":2,\"Fair\":1}\n",
    "d_color = {\"D\":7,\"E\":6,\"F\":5,\"G\":4,\"H\":3,\"I\":2,\"J\":1}\n",
    "d_clarity = {\"IF\":8,\"VVS1\":7,\"VVS2\":6,\"VS1\":5,\"VS2\":4,\"SI1\":3,\"SI2\":2,\"I1\":4}\n",
    " \n",
    "\n",
    "\n",
    "data[\"cut\"] = data[\"cut\"].map(d_cut)\n",
    "data[\"color\"] = data[\"color\"].map(d_color)\n",
    "data[\"clarity\"] = data[\"clarity\"].map(d_clarity)\n",
    "\n",
    "\n",
    "max_price_global = data[\"price\"].max()\n",
    "\n",
    "n = data.values.astype(numpy.double)\n",
    "\n",
    "n = n / n.max(axis=0) #normalise data\n",
    "\n",
    "X_train , X_test = train_test_split(n, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.8:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "selected_columns = data.columns[columns]\n",
    "data = data[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     carat  cut  color  clarity  depth  table  price     x\n0     1.03    5      7        8   62.0   56.0  17590  6.55\n1     2.53    4      3        2   59.6   59.0  17591  8.90\n2     2.01    4      2        4   61.1   60.0  17592  8.12\n3     1.71    5      4        5   62.2   55.0  17595  7.72\n4     1.70    3      5        4   60.8   60.0  17597  7.65\n..     ...  ...    ...      ...    ...    ...    ...   ...\n418   1.60    5      5        5   60.5   57.0  18571  7.60\n419   2.01    2      4        3   63.1   59.0  18572  7.98\n420   2.01    1      4        3   70.6   64.0  18574  7.43\n421   2.11    4      7        2   60.9   60.0  18575  8.28\n422   2.03    5      5        2   61.6   55.0  18578  8.19\n\n[423 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8\n423\n"
     ]
    }
   ],
   "source": [
    "iterations = 1000\n",
    "learning_rate = 0.01\n",
    "rows , cols = data.shape\n",
    "print(cols)\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss is  [95 95 33]\nweight is  [6 7 8 9 3]\n(3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def cost_function(X, Y, B):\n",
    "    m = len(Y)\n",
    "    J = X.dot(W)\n",
    "    return J\n",
    "\n",
    "X = np.array([[1, 2, 3, 4, 5],[1, 2, 3, 4, 5],[1, 1, 1, 1, 1]])\n",
    "\n",
    "w = np.array([6,7,8,9,3])\n",
    "loss = X.dot(w)\n",
    "print(\"loss is \",loss)\n",
    "print(\"weight is \",w)\n",
    "m  = loss.T.shape\n",
    "print (m)\n",
    "# g = X.dot(loss)\n",
    "# print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-1.63031974  1.07924945  1.92493008 ...  0.26539485 -0.8853912\n  -1.1868065 ]\n [ 1.06177623  0.13406825 -0.49302218 ... -1.27684244  0.50756867\n   0.93809444]\n [ 0.12851629  0.13406825 -1.09751025 ... -0.31294413  0.97188862\n   0.23280817]\n ...\n [ 0.12851629 -2.70147532  0.11146588 ...  5.79174516  2.82916845\n  -0.39109891]\n [ 0.30798936  0.13406825  1.92493008 ... -0.44146391  0.97188862\n   0.37748227]\n [ 0.1644109   1.07924945  0.71595395 ...  0.0083553  -1.34971115\n   0.29610309]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "headers = [\"carat\",\t\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"]\n",
    "data = pd.read_csv('diamonds1.1.csv', na_values='?',    \n",
    "         header=None,  names = headers) \n",
    "#data = data.reset_index(drop=True)\n",
    "#print(type(data))\n",
    "\n",
    "data = data.iloc[1:,:]\n",
    "\n",
    "#print(type(data))\n",
    "\n",
    "d_cut = {\"Ideal\":5,\"Premium\":4,\"Very Good\":3,\"Good\":2,\"Fair\":1}\n",
    "d_color = {\"D\":7,\"E\":6,\"F\":5,\"G\":4,\"H\":3,\"I\":2,\"J\":1}\n",
    "d_clarity = {\"IF\":8,\"VVS1\":7,\"VVS2\":6,\"VS1\":5,\"VS2\":4,\"SI1\":3,\"SI2\":2,\"I1\":4}\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "data[\"cut\"] = data[\"cut\"].map(d_cut)\n",
    "data[\"color\"] = data[\"color\"].map(d_color)\n",
    "data[\"clarity\"] = data[\"clarity\"].map(d_clarity)\n",
    "\n",
    "X = data[['carat','cut','color','clarity','depth','table','x']]\n",
    "Y = data[['price']]\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train is  [[1 2 3 4 5]]\ngradient length  6\ngradient width  1\n[[ -87.95]\n [ -87.95]\n [-175.9 ]\n [-263.85]\n [-351.8 ]\n [-439.75]]\ngradient length  6\ngradient width  6\n[[ -63.324  -63.324  -63.324  -63.324  -63.324  -63.324]\n [ -63.324  -63.324  -63.324  -63.324  -63.324  -63.324]\n [-126.648 -126.648 -126.648 -126.648 -126.648 -126.648]\n [-189.972 -189.972 -189.972 -189.972 -189.972 -189.972]\n [-253.296 -253.296 -253.296 -253.296 -253.296 -253.296]\n [-316.62  -316.62  -316.62  -316.62  -316.62  -316.62 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy\n",
    "#print(type(data))\n",
    "\n",
    "\n",
    "\n",
    "# print(X)\n",
    "# X_final = X.values.astype(np.double)\n",
    "# Y_final = Y.values.astype(np.double)\n",
    "def cost_function(X, Y, B):\n",
    "    m = len(Y)\n",
    "    J = np.sum((X.dot(B) - Y) ** 2)/(2 * m)\n",
    "    return J\n",
    "\n",
    "\n",
    "def batch_gradient_descent(X, Y, B, alpha, iterations):\n",
    "    cost_history = [0] * iterations\n",
    "    m = len(Y)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "    #print(iteration)\n",
    "    # Hypothesis Values\n",
    "        h = X.dot(B)\n",
    "        # Difference b/w Hypothesis and Actual Y\n",
    "        loss = h - Y\n",
    "        # Gradient Calculation\n",
    "        gradient = X.T.dot(loss) / m\n",
    "        gl,gw = gradient.shape\n",
    "        print(\"gradient length \",gl)\n",
    "        print(\"gradient width \",gw)\n",
    "        # Changing Values of B using Gradient\n",
    "        temp = alpha * gradient\n",
    "        print(temp)\n",
    "        B = B - temp\n",
    "        print(\" updated B is \",B)\n",
    "        # New Cost Value\n",
    "        #cost = cost_function(X, Y, B)\n",
    "        #cost_history[iteration] = cost\n",
    "        \n",
    "    return B, cost_history\n",
    "\n",
    "m = 1\n",
    "f = 7\n",
    "X_train = X[:m]\n",
    "\n",
    "print(\"X_train is \",X_train)\n",
    "\n",
    "X_train = np.c_[np.ones(len(X_train),dtype='int32'),X_train]\n",
    "y_train = Y_final[:m]\n",
    "X_test = X_final[m:]\n",
    "X_test = np.c_[np.ones(len(X_test),dtype='int32'),X_test]\n",
    "y_test = Y_final[m:]\n",
    "\n",
    "# Initial Coefficients\n",
    "B = np.zeros(X_train.shape[1])\n",
    "alpha = 0.005\n",
    "iter_ = 2\n",
    "newB, cost_history = batch_gradient_descent(X_train, y_train, B, alpha, iter_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  908.50669075   908.50669075   908.50669075   908.50669075\n    908.50669075   908.50669075   908.50669075   908.50669075]\n [-1481.15639305 -1481.15639305 -1481.15639305 -1481.15639305\n  -1481.15639305 -1481.15639305 -1481.15639305 -1481.15639305]\n [  980.50534218   980.50534218   980.50534218   980.50534218\n    980.50534218   980.50534218   980.50534218   980.50534218]\n [ 1748.81186094  1748.81186094  1748.81186094  1748.81186094\n   1748.81186094  1748.81186094  1748.81186094  1748.81186094]\n [ 2659.69164484  2659.69164484  2659.69164484  2659.69164484\n   2659.69164484  2659.69164484  2659.69164484  2659.69164484]\n [  241.113        241.113        241.113        241.113\n    241.113        241.113        241.113        241.113     ]\n [ -804.38382533  -804.38382533  -804.38382533  -804.38382533\n   -804.38382533  -804.38382533  -804.38382533  -804.38382533]\n [-1078.22164166 -1078.22164166 -1078.22164166 -1078.22164166\n  -1078.22164166 -1078.22164166 -1078.22164166 -1078.22164166]]\n"
     ]
    }
   ],
   "source": [
    "print(newB)\n",
    "weights=[]\n",
    "for i in range(0,len(newB)):\n",
    "    weights.append(newB[i][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-1.63031974e+00  1.07924945e+00  1.92493008e+00  2.92754217e+00\n   2.65394853e-01 -8.85391196e-01 -1.18680650e+00]\n [ 1.06177623e+00  1.34068254e-01 -4.93022182e-01 -1.00372874e+00\n  -1.27684244e+00  5.07568670e-01  9.38094435e-01]\n [ 1.28516291e-01  1.34068254e-01 -1.09751025e+00  3.06694894e-01\n  -3.12944132e-01  9.71888625e-01  2.32808169e-01]\n [-4.09902902e-01  1.07924945e+00  1.11465885e-01  9.61906712e-01\n   3.93914628e-01 -1.34971115e+00 -1.28877096e-01]\n [-4.27850209e-01 -8.11112937e-01  7.15953951e-01  3.06694894e-01\n  -5.05723793e-01  9.71888625e-01 -1.92172017e-01]\n [-4.27850209e-01 -1.75629413e+00  7.15953951e-01  3.06694894e-01\n   3.93914628e-01 -8.85391196e-01 -2.91635465e-01]\n [-1.00216402e+00 -8.11112937e-01  7.15953951e-01  2.92754217e+00\n  -2.17648086e+00  9.71888625e-01 -4.90562361e-01]\n [ 1.10568985e-01 -1.75629413e+00 -4.93022182e-01 -3.48516924e-01\n   1.10077339e+00  5.07568670e-01  8.81340630e-02]\n [-5.35534047e-01  1.07924945e+00  1.11465885e-01  1.61711853e+00\n   1.36875079e-01 -8.85391196e-01 -2.46424807e-01]\n [-3.91955596e-01  1.34068254e-01  1.11465885e-01  9.61906712e-01\n  -1.40536221e+00  9.71888625e-01 -3.84557796e-02]\n [ 2.90042049e-01  1.34068254e-01 -1.70199832e+00  2.27233035e+00\n  -1.84424357e-01  4.32487145e-02  3.86524407e-01]\n [ 1.10568985e-01  1.07924945e+00 -1.09751025e+00  9.61906712e-01\n   2.65394853e-01 -4.21071241e-01  1.60471116e-01]\n [ 9.72039695e-01  1.34068254e-01 -1.09751025e+00 -1.00372874e+00\n   9.07993726e-01  1.90052854e+00  6.84914750e-01]\n [ 1.04382892e+00 -1.75629413e+00  1.11465885e-01 -1.00372874e+00\n   1.35781294e+00  5.07568670e-01  6.39704092e-01]\n [ 1.28516291e-01  1.07924945e+00  1.11465885e-01 -1.00372874e+00\n  -4.41463906e-01 -1.34971115e+00  2.68976695e-01]\n [-3.56060983e-01  1.34068254e-01  1.11465885e-01  3.06694894e-01\n  -3.77204019e-01  4.32487145e-02 -8.36664377e-02]\n [ 2.36200130e-01  1.07924945e+00  1.11465885e-01 -1.00372874e+00\n   3.93914628e-01 -1.81403111e+00  2.68976695e-01]\n [ 2.54147437e-01  1.34068254e-01 -4.93022182e-01 -3.48516924e-01\n   6.50954177e-01  5.07568670e-01  2.14723906e-01]\n [ 4.69515114e-01  1.34068254e-01 -4.93022182e-01 -1.00372874e+00\n   7.15214064e-01  4.32487145e-02  4.22692933e-01]\n [ 2.00305517e-01  1.34068254e-01 -1.09751025e+00  9.61906712e-01\n  -6.34243568e-01  9.71888625e-01  3.14187354e-01]\n [ 8.10513937e-01  1.34068254e-01 -1.09751025e+00  9.61906712e-01\n  -2.48684244e-01  9.71888625e-01  6.93956882e-01]\n [-7.68849031e-01  1.07924945e+00  1.32044202e+00  9.61906712e-01\n   3.93914628e-01 -4.21071241e-01 -4.45351703e-01]\n [ 2.18252824e-01  1.34068254e-01  1.11465885e-01 -1.00372874e+00\n   7.15214064e-01  4.32487145e-02  2.32808169e-01]\n [ 1.46463598e-01 -8.11112937e-01  7.15953951e-01 -3.48516924e-01\n  -9.55543004e-01  9.71888625e-01  3.14187354e-01]\n [-7.50901725e-01  1.07924945e+00  1.92493008e+00  9.61906712e-01\n  -2.48684244e-01 -4.21071241e-01 -4.18225308e-01]\n [-4.27850209e-01  1.07924945e+00  7.15953951e-01  3.06694894e-01\n  -1.84424357e-01 -1.81403111e+00 -1.28877096e-01]\n [-1.07395324e+00  1.34068254e-01  1.32044202e+00  2.92754217e+00\n   1.36875079e-01  4.32487145e-02 -6.44278598e-01]\n [ 6.31040872e-01  1.34068254e-01 -4.93022182e-01 -1.00372874e+00\n   5.86694290e-01  4.32487145e-02  5.13114249e-01]\n [-7.86796338e-01  1.34068254e-01  1.32044202e+00  1.61711853e+00\n  -1.85518143e+00  1.43620858e+00 -3.36846123e-01]\n [ 1.46463598e-01 -8.11112937e-01 -4.93022182e-01 -3.48516924e-01\n   1.16503327e+00  9.71888625e-01  8.81340630e-02]\n [ 6.13093566e-01  1.34068254e-01 -1.09751025e+00 -3.48516924e-01\n   8.35530449e-03  4.32487145e-02  5.13114249e-01]\n [ 3.61831275e-01  1.34068254e-01  1.11465885e-01 -1.00372874e+00\n  -7.62763342e-01  5.07568670e-01  4.67903591e-01]\n [ 1.10568985e-01  1.34068254e-01  1.92493008e+00 -1.00372874e+00\n   8.35530449e-03  5.07568670e-01  2.05681774e-01]\n [ 1.28516291e-01 -8.11112937e-01  1.32044202e+00 -3.48516924e-01\n   7.15214064e-01 -4.21071241e-01  1.51428984e-01]\n [ 6.84882792e-01  1.07924945e+00 -1.09751025e+00 -1.00372874e+00\n   2.65394853e-01 -4.21071241e-01  5.67367039e-01]\n [-4.09902902e-01 -8.11112937e-01  7.15953951e-01  9.61906712e-01\n   2.01134966e-01  1.43620858e+00 -2.28340544e-01]\n [ 1.28516291e-01  1.07924945e+00  1.32044202e+00 -1.00372874e+00\n   3.93914628e-01 -4.21071241e-01  1.60471116e-01]\n [ 1.28516291e-01  1.34068254e-01 -4.93022182e-01 -3.48516924e-01\n   7.15214064e-01  5.07568670e-01  1.51428984e-01]\n [-7.68849031e-01 -8.11112937e-01  1.32044202e+00  1.61711853e+00\n  -8.91283117e-01  1.43620858e+00 -4.72478097e-01]\n [ 1.04382892e+00 -8.11112937e-01  1.11465885e-01 -1.00372874e+00\n   1.03651350e+00  4.32487145e-02  7.12041145e-01]\n [ 1.46463598e-01  1.34068254e-01  1.11465885e-01 -3.48516924e-01\n   7.79473951e-01  9.71888625e-01  1.42386853e-01]\n [ 3.25936662e-01 -8.11112937e-01  7.15953951e-01 -1.00372874e+00\n  -6.34243568e-01  4.32487145e-02  3.68440143e-01]\n [ 1.10568985e-01  1.07924945e+00  7.15953951e-01 -1.00372874e+00\n   2.65394853e-01 -4.21071241e-01  1.51428984e-01]\n [ 3.07989356e-01  1.07924945e+00 -1.09751025e+00  3.06694894e-01\n   3.93914628e-01 -1.34971115e+00  3.05145222e-01]\n [ 9.26216785e-02 -2.70147532e+00 -4.93022182e-01  3.06694894e-01\n  -6.98503455e-01  2.36484849e+00  3.05145222e-01]\n [ 4.87462421e-01  1.34068254e-01 -1.09751025e+00 -3.48516924e-01\n  -3.12944132e-01  5.07568670e-01  5.04072118e-01]\n [ 6.66935485e-01 -8.11112937e-01 -1.09751025e+00 -3.48516924e-01\n   5.86694290e-01 -1.34971115e+00  4.76945723e-01]\n [ 1.10568985e-01  1.34068254e-01 -1.09751025e+00  3.06694894e-01\n   1.36875079e-01 -8.85391196e-01  2.23766037e-01]\n [-7.50901725e-01  1.34068254e-01  1.92493008e+00  9.61906712e-01\n  -3.77204019e-01  5.07568670e-01 -3.45888255e-01]\n [ 1.10568985e-01 -8.11112937e-01  1.11465885e-01 -3.48516924e-01\n   9.07993726e-01 -1.34971115e+00  8.81340630e-02]\n [-5.17586741e-01  1.07924945e+00  1.11465885e-01  2.92754217e+00\n   5.22434402e-01 -1.34971115e+00 -2.28340544e-01]\n [ 3.25936662e-01  1.07924945e+00  7.15953951e-01 -1.00372874e+00\n   4.58174515e-01 -4.21071241e-01  2.96103090e-01]\n [ 1.10568985e-01 -1.75629413e+00  1.11465885e-01 -3.48516924e-01\n   1.67911237e+00  5.07568670e-01 -2.03715164e-02]\n [ 3.61831275e-01  1.34068254e-01  7.15953951e-01 -1.00372874e+00\n  -2.17648086e+00  1.43620858e+00  5.76409171e-01]\n [ 1.28516291e-01  1.34068254e-01  7.15953951e-01 -1.00372874e+00\n  -1.84424357e-01  1.43620858e+00  1.42386853e-01]\n [ 4.15673195e-01  1.07924945e+00 -1.09751025e+00 -3.48516924e-01\n  -5.69983681e-01 -4.21071241e-01  4.67903591e-01]\n [ 1.28516291e-01 -8.11112937e-01 -1.09751025e+00  9.61906712e-01\n  -1.21258255e+00  2.36484849e+00  3.41313748e-01]\n [ 1.64410904e-01 -8.11112937e-01  1.11465885e-01 -1.00372874e+00\n   7.15214064e-01 -1.34971115e+00  1.87597511e-01]\n [ 1.11561815e+00  1.07924945e+00 -1.70199832e+00 -3.48516924e-01\n  -1.20164470e-01  4.32487145e-02  7.84378198e-01]\n [ 1.28516291e-01 -8.11112937e-01  1.32044202e+00 -1.00372874e+00\n  -1.46962210e+00  1.43620858e+00  2.50892432e-01]\n [ 1.28516291e-01 -8.11112937e-01 -4.93022182e-01 -3.48516924e-01\n   7.79473951e-01  5.07568670e-01  1.15260458e-01]\n [ 1.10568985e-01  1.34068254e-01  1.32044202e+00 -1.00372874e+00\n   3.93914628e-01 -8.85391196e-01  2.50892432e-01]\n [ 1.10568985e-01 -8.11112937e-01 -4.93022182e-01 -3.48516924e-01\n   1.10077339e+00  5.07568670e-01  1.51428984e-01]\n [ 1.10568985e-01 -1.75629413e+00  1.32044202e+00 -1.00372874e+00\n  -2.49778030e+00  1.43620858e+00  4.04608670e-01]\n [ 1.10568985e-01 -1.75629413e+00  1.92493008e+00  3.06694894e-01\n   1.35781294e+00  9.71888625e-01  1.57970101e-02]\n [ 1.10568985e-01  1.07924945e+00  1.11465885e-01  3.06694894e-01\n   4.58174515e-01 -4.21071241e-01  1.78555379e-01]\n [ 1.10568985e-01  1.34068254e-01 -4.93022182e-01 -3.48516924e-01\n   6.50954177e-01 -8.85391196e-01  1.33344721e-01]\n [-1.22745999e-01  1.07924945e+00 -4.93022182e-01  9.61906712e-01\n  -1.21258255e+00  9.71888625e-01  1.06218326e-01]\n [-3.91955596e-01  1.34068254e-01  1.11465885e-01  9.61906712e-01\n  -1.40536221e+00  9.71888625e-01 -2.28725315e-03]\n [-4.09902902e-01  1.34068254e-01  7.15953951e-01  3.06694894e-01\n  -1.20164470e-01  9.71888625e-01 -1.28877096e-01]]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (8,) (8,70) ",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-d18f0b1c6ddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0miter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mnewB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-31029637a1e3>\u001b[0m in \u001b[0;36mbatch_gradient_descent\u001b[0;34m(X, Y, B, alpha, iterations)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Changing Values of B using Gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m# New Cost Value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (8,) (8,70) "
     ]
    }
   ],
   "source": [
    "m = 70\n",
    "f = 7\n",
    "X_train = X[:m]\n",
    "print(X_train)\n",
    "X_train = np.c_[np.ones(len(X_train),dtype='int32'),X_train]\n",
    "y_train = Y_final[:m]\n",
    "X_test = X_final[m:]\n",
    "X_test = np.c_[np.ones(len(X_test),dtype='int32'),X_test]\n",
    "y_test = Y_final[m:]\n",
    "\n",
    "# Initial Coefficients\n",
    "B = np.zeros(X_train.shape[1])\n",
    "alpha = 0.005\n",
    "iter_ = 2000\n",
    "newB, cost_history = batch_gradient_descent(X_train, y_train, B, alpha, iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(y_,y):\n",
    " sst = np.sum((y-y.mean())**2)\n",
    " ssr = np.sum((y_-y)**2) \n",
    " r2 = 1-(ssr/sst)\n",
    " return(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0. 0. 0.]\n[1.76737899e+04 9.54549977e+01 9.47673413e-01]\n3\n"
     ]
    }
   ],
   "source": [
    "print(B)\n",
    "print(newB)\n",
    "print(len(newB))\n",
    "\n",
    "# for x in X_test:\n",
    "#     print(x.dot(newB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}